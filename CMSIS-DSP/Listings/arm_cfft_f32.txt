; generated by Component: ARM Compiler 5.06 update 4 (build 422) Tool: ArmCC [4d3604]
; commandline ArmCC [--c99 --list --split_sections --debug -c --asm --interleave -o.\objects\arm_cfft_f32.o --asm_dir=.\Listings\ --list_dir=.\Listings\ --depend=.\objects\arm_cfft_f32.d --cpu=Cortex-M4.fp --apcs=interwork -O3 --diag_suppress=9931 -I.\inc -I"C:\Users\emh203\Google Drive\Teaching\EE403W\Spring 2017\Labs\Lab 4\CMSIS-DSP\RTE\_CMSIS_DSP_4_5_O3" -IC:\Keil_v5\ARM\PACK\ARM\CMSIS\5.0.0\Device\ARM\ARMCM4\Include -IC:\Keil_v5\ARM\CMSIS\Include -D__UVISION_VERSION=522 -DARMCM4_FP -D__FPU_PRESENT=1 -DARM_MATH_CM4 -D__CC_ARM --omf_browse=.\objects\arm_cfft_f32.crf src\TransformFunctions\arm_cfft_f32.c]
                          THUMB

                          AREA ||i.arm_cfft_f32||, CODE, READONLY, ALIGN=1

                  arm_cfft_f32 PROC
;;;575    
;;;576    void arm_cfft_f32( 
000000  e92d41f0          PUSH     {r4-r8,lr}
;;;577        const arm_cfft_instance_f32 * S, 
;;;578        float32_t * p1,
;;;579        uint8_t ifftFlag,
;;;580        uint8_t bitReverseFlag)
;;;581    {
;;;582        uint32_t  L = S->fftLen, l;
000004  8805              LDRH     r5,[r0,#0]
000006  4698              MOV      r8,r3                 ;581
000008  4617              MOV      r7,r2                 ;581
00000a  460c              MOV      r4,r1                 ;581
00000c  4606              MOV      r6,r0                 ;581
;;;583        float32_t invL, * pSrc;
;;;584    
;;;585        if(ifftFlag == 1u)
00000e  2a01              CMP      r2,#1
000010  d10c              BNE      |L1.44|
;;;586        {
;;;587            /*  Conjugate input data  */
;;;588            pSrc = p1 + 1;
000012  1d20              ADDS     r0,r4,#4
;;;589            for(l=0; l<L; l++) 
000014  2100              MOVS     r1,#0
000016  e007              B        |L1.40|
                  |L1.24|
;;;590            {
;;;591                *pSrc = -*pSrc;
000018  ed900a00          VLDR     s0,[r0,#0]
00001c  eeb10a40          VNEG.F32 s0,s0
000020  ed800a00          VSTR     s0,[r0,#0]
000024  3008              ADDS     r0,r0,#8
000026  1c49              ADDS     r1,r1,#1
                  |L1.40|
000028  42a9              CMP      r1,r5                 ;589
00002a  d3f5              BCC      |L1.24|
                  |L1.44|
;;;592                pSrc += 2;
;;;593            }
;;;594        }
;;;595    
;;;596        switch (L) 
00002c  f5b57f80          CMP      r5,#0x100
000030  d01b              BEQ      |L1.106|
000032  dc08              BGT      |L1.70|
000034  2d10              CMP      r5,#0x10
000036  d013              BEQ      |L1.96|
000038  2d20              CMP      r5,#0x20
00003a  d016              BEQ      |L1.106|
00003c  2d40              CMP      r5,#0x40
00003e  d019              BEQ      |L1.116|
000040  2d80              CMP      r5,#0x80
000042  d11d              BNE      |L1.128|
000044  e00c              B        |L1.96|
                  |L1.70|
000046  f5b57f00          CMP      r5,#0x200
00004a  d013              BEQ      |L1.116|
00004c  f5b56f80          CMP      r5,#0x400
000050  d006              BEQ      |L1.96|
000052  f5b56f00          CMP      r5,#0x800
000056  d008              BEQ      |L1.106|
000058  f5b55f80          CMP      r5,#0x1000
00005c  d110              BNE      |L1.128|
00005e  e009              B        |L1.116|
                  |L1.96|
;;;597        {
;;;598        case 16: 
;;;599        case 128:
;;;600        case 1024:
;;;601            arm_cfft_radix8by2_f32  ( (arm_cfft_instance_f32 *) S, p1);
000060  4621              MOV      r1,r4
000062  4630              MOV      r0,r6
000064  f7fffffe          BL       arm_cfft_radix8by2_f32
;;;602            break;
000068  e00a              B        |L1.128|
                  |L1.106|
;;;603        case 32:
;;;604        case 256:
;;;605        case 2048:
;;;606            arm_cfft_radix8by4_f32  ( (arm_cfft_instance_f32 *) S, p1);
00006a  4621              MOV      r1,r4
00006c  4630              MOV      r0,r6
00006e  f7fffffe          BL       arm_cfft_radix8by4_f32
;;;607            break;
000072  e005              B        |L1.128|
                  |L1.116|
;;;608        case 64:
;;;609        case 512:
;;;610        case 4096:
;;;611            arm_radix8_butterfly_f32( p1, L, (float32_t *) S->pTwiddle, 1);
000074  4629              MOV      r1,r5
000076  2301              MOVS     r3,#1
000078  4620              MOV      r0,r4
00007a  6872              LDR      r2,[r6,#4]
00007c  f7fffffe          BL       arm_radix8_butterfly_f32
                  |L1.128|
;;;612            break;
;;;613        }  
;;;614    
;;;615        if( bitReverseFlag )
000080  f1b80f00          CMP      r8,#0
000084  d004              BEQ      |L1.144|
;;;616            arm_bitreversal_32((uint32_t*)p1,S->bitRevLength,S->pBitRevTable);
000086  89b1              LDRH     r1,[r6,#0xc]
000088  4620              MOV      r0,r4
00008a  68b2              LDR      r2,[r6,#8]
00008c  f7fffffe          BL       arm_bitreversal_32
                  |L1.144|
;;;617    
;;;618        if(ifftFlag == 1u)
000090  2f01              CMP      r7,#1
000092  d119              BNE      |L1.200|
;;;619        {
;;;620            invL = 1.0f/(float32_t)L;
000094  ee005a10          VMOV     s0,r5
000098  eeb71a00          VMOV.F32 s2,#1.00000000
;;;621            /*  Conjugate and scale output data */
;;;622            pSrc = p1;
;;;623            for(l=0; l<L; l++) 
00009c  2000              MOVS     r0,#0
00009e  eef80a40          VCVT.F32.U32 s1,s0                 ;620
0000a2  ee810a20          VDIV.F32 s0,s2,s1              ;620
0000a6  e00d              B        |L1.196|
                  |L1.168|
;;;624            {
;;;625                *pSrc++ *=   invL ;
0000a8  edd40a00          VLDR     s1,[r4,#0]
;;;626                *pSrc  = -(*pSrc) * invL;
0000ac  1c40              ADDS     r0,r0,#1
0000ae  ee600a80          VMUL.F32 s1,s1,s0              ;625
0000b2  edc40a00          VSTR     s1,[r4,#0]            ;625
0000b6  edd40a01          VLDR     s1,[r4,#4]
0000ba  ee600ac0          VNMUL.F32 s1,s1,s0
0000be  edc40a01          VSTR     s1,[r4,#4]
0000c2  3408              ADDS     r4,r4,#8
                  |L1.196|
0000c4  42a8              CMP      r0,r5                 ;623
0000c6  d3ef              BCC      |L1.168|
                  |L1.200|
;;;627                pSrc++;
;;;628            }
;;;629        }
;;;630    }
0000c8  e8bd81f0          POP      {r4-r8,pc}
;;;631    
                          ENDP


                          AREA ||i.arm_cfft_radix8by2_f32||, CODE, READONLY, ALIGN=1

                  arm_cfft_radix8by2_f32 PROC
;;;208    
;;;209    void arm_cfft_radix8by2_f32( arm_cfft_instance_f32 * S, float32_t * p1) 
000000  e92d41f0          PUSH     {r4-r8,lr}
;;;210    {
000004  4605              MOV      r5,r0
000006  ed2d8b02          VPUSH    {d8}
;;;211        uint32_t    L  = S->fftLen;
;;;212        float32_t * pCol1, * pCol2, * pMid1, * pMid2;
;;;213        float32_t * p2 = p1 + L;
;;;214        const float32_t * tw = (float32_t *) S->pTwiddle;
;;;215        float32_t t1[4], t2[4], t3[4], t4[4], twR, twI;
;;;216        float32_t m0, m1, m2, m3;
;;;217        uint32_t l;
;;;218    
;;;219        pCol1 = p1;
00000a  468c              MOV      r12,r1
00000c  8802              LDRH     r2,[r0,#0]            ;211
;;;220        pCol2 = p2;
;;;221    
;;;222        //    Define new length
;;;223        L >>= 1;
;;;224        //    Initialize mid pointers
;;;225        pMid1 = p1 + L;
;;;226        pMid2 = p2 + L;
;;;227    
;;;228        // do two dot Fourier transform
;;;229        for ( l = L >> 2; l > 0; l-- ) 
00000e  686e              LDR      r6,[r5,#4]
000010  eb010082          ADD      r0,r1,r2,LSL #2       ;213
000014  0854              LSRS     r4,r2,#1              ;223
000016  4680              MOV      r8,r0                 ;220
000018  eb010284          ADD      r2,r1,r4,LSL #2       ;225
00001c  eb000384          ADD      r3,r0,r4,LSL #2       ;226
000020  08a7              LSRS     r7,r4,#2
000022  d07c              BEQ      |L2.286|
                  |L2.36|
;;;230        {
;;;231            t1[0] = p1[0];
000024  ed913a00          VLDR     s6,[r1,#0]
;;;232            t1[1] = p1[1];
;;;233            t1[2] = p1[2];
;;;234            t1[3] = p1[3];
;;;235    
;;;236            t2[0] = p2[0];
000028  edd02a00          VLDR     s5,[r0,#0]
00002c  ed915a01          VLDR     s10,[r1,#4]           ;232
;;;237            t2[1] = p2[1];
000030  ed904a01          VLDR     s8,[r0,#4]
;;;238            t2[2] = p2[2];
;;;239            t2[3] = p2[3];
;;;240    
;;;241            t3[0] = pMid1[0];
;;;242            t3[1] = pMid1[1];
;;;243            t3[2] = pMid1[2];
;;;244            t3[3] = pMid1[3];
;;;245    
;;;246            t4[0] = pMid2[0];
;;;247            t4[1] = pMid2[1];
;;;248            t4[2] = pMid2[2];
;;;249            t4[3] = pMid2[3];
;;;250    
;;;251            *p1++ = t1[0] + t2[0];
000034  ee338a22          VADD.F32 s16,s6,s5
000038  edd17a02          VLDR     s15,[r1,#8]           ;233
00003c  ed911a03          VLDR     s2,[r1,#0xc]          ;234
000040  ed902a02          VLDR     s4,[r0,#8]            ;238
000044  edd03a03          VLDR     s7,[r0,#0xc]          ;239
000048  ed927a00          VLDR     s14,[r2,#0]           ;241
00004c  edd26a01          VLDR     s13,[r2,#4]           ;242
000050  edd21a02          VLDR     s3,[r2,#8]            ;243
000054  edd24a03          VLDR     s9,[r2,#0xc]          ;244
000058  edd30a00          VLDR     s1,[r3,#0]            ;246
00005c  ed930a01          VLDR     s0,[r3,#4]            ;247
000060  edd35a02          VLDR     s11,[r3,#8]           ;248
000064  ed936a03          VLDR     s12,[r3,#0xc]         ;249
000068  ed818a00          VSTR     s16,[r1,#0]
;;;252            *p1++ = t1[1] + t2[1];
00006c  ee358a04          VADD.F32 s16,s10,s8
;;;253            *p1++ = t1[2] + t2[2];
;;;254            *p1++ = t1[3] + t2[3];    // col 1
;;;255    
;;;256            t2[0] = t1[0] - t2[0];
000070  ee333a62          VSUB.F32 s6,s6,s5
;;;257            t2[1] = t1[1] - t2[1];
000074  ee752a44          VSUB.F32 s5,s10,s8
000078  ed818a01          VSTR     s16,[r1,#4]           ;252
00007c  ee378a82          VADD.F32 s16,s15,s4            ;253
;;;258            t2[2] = t1[2] - t2[2];
;;;259            t2[3] = t1[3] - t2[3];    // for col 2
000080  ee315a63          VSUB.F32 s10,s2,s7
000084  ee374ac2          VSUB.F32 s8,s15,s4             ;258
000088  ed818a02          VSTR     s16,[r1,#8]           ;253
00008c  ee318a23          VADD.F32 s16,s2,s7             ;254
;;;260    
;;;261            *pMid1++ = t3[0] + t4[0];
000090  ee371a20          VADD.F32 s2,s14,s1
;;;262            *pMid1++ = t3[1] + t4[1];
;;;263            *pMid1++ = t3[2] + t4[2];
;;;264            *pMid1++ = t3[3] + t4[3]; // col 1
;;;265    
;;;266            t4[0] = t4[0] - t3[0];
000094  ee302ac7          VSUB.F32 s4,s1,s14
000098  ed818a03          VSTR     s16,[r1,#0xc]         ;254
00009c  ed821a00          VSTR     s2,[r2,#0]            ;261
0000a0  ee361a80          VADD.F32 s2,s13,s0             ;262
;;;267            t4[1] = t4[1] - t3[1];
;;;268            t4[2] = t4[2] - t3[2];
;;;269            t4[3] = t4[3] - t3[3];    // for col 2
0000a4  ee763a64          VSUB.F32 s7,s12,s9
0000a8  ed821a01          VSTR     s2,[r2,#4]            ;262
0000ac  ee311aa5          VADD.F32 s2,s3,s11             ;263
0000b0  ee751ae1          VSUB.F32 s3,s11,s3             ;268
0000b4  ed821a02          VSTR     s2,[r2,#8]            ;263
0000b8  ee341a86          VADD.F32 s2,s9,s12             ;264
0000bc  ed821a03          VSTR     s2,[r2,#0xc]          ;264
0000c0  ee301a66          VSUB.F32 s2,s0,s13             ;267
;;;270    
;;;271            twR = *tw++;
0000c4  ed960a00          VLDR     s0,[r6,#0]
;;;272            twI = *tw++;
0000c8  edd60a01          VLDR     s1,[r6,#4]
;;;273    
;;;274            // multiply by twiddle factors
;;;275            m0 = t2[0] * twR;
0000cc  ee634a00          VMUL.F32 s9,s6,s0
;;;276            m1 = t2[1] * twI;
0000d0  ee625aa0          VMUL.F32 s11,s5,s1
;;;277            m2 = t2[1] * twR;
0000d4  ee622a80          VMUL.F32 s5,s5,s0
;;;278            m3 = t2[0] * twI;
0000d8  ee233a20          VMUL.F32 s6,s6,s1
;;;279            
;;;280            // R  =  R  *  Tr - I * Ti
;;;281            *p2++ = m0 + m1;
0000dc  ee744aa5          VADD.F32 s9,s9,s11
;;;282            // I  =  I  *  Tr + R * Ti
;;;283            *p2++ = m2 - m3;
0000e0  ee722ac3          VSUB.F32 s5,s5,s6
0000e4  edc04a00          VSTR     s9,[r0,#0]            ;281
;;;284            
;;;285            // use vertical symmetry
;;;286            //  0.9988 - 0.0491i <==> -0.0491 - 0.9988i
;;;287            m0 = t4[0] * twI;
0000e8  ee223a20          VMUL.F32 s6,s4,s1
0000ec  edc02a01          VSTR     s5,[r0,#4]            ;283
;;;288            m1 = t4[1] * twR;
0000f0  ee612a00          VMUL.F32 s5,s2,s0
;;;289            m2 = t4[1] * twI;
0000f4  ee610a20          VMUL.F32 s1,s2,s1
;;;290            m3 = t4[0] * twR;
0000f8  ee220a00          VMUL.F32 s0,s4,s0
;;;291            
;;;292            *pMid2++ = m0 - m1;
0000fc  ee331a62          VSUB.F32 s2,s6,s5
;;;293            *pMid2++ = m2 + m3;
000100  ee300a80          VADD.F32 s0,s1,s0
000104  ed831a00          VSTR     s2,[r3,#0]            ;292
000108  ed830a01          VSTR     s0,[r3,#4]
;;;294    
;;;295            twR = *tw++;
00010c  ed960a02          VLDR     s0,[r6,#8]
;;;296            twI = *tw++;
000110  edd60a03          VLDR     s1,[r6,#0xc]
;;;297            
;;;298            m0 = t2[2] * twR;
000114  ee242a00          VMUL.F32 s4,s8,s0
;;;299            m1 = t2[3] * twI;
000118  ee251a20          VMUL.F32 s2,s10,s1
;;;300            m2 = t2[3] * twR;
00011c  e000              B        |L2.288|
                  |L2.286|
00011e  e023              B        |L2.360|
                  |L2.288|
000120  ee652a00          VMUL.F32 s5,s10,s0
;;;301            m3 = t2[2] * twI;
000124  ee243a20          VMUL.F32 s6,s8,s1
;;;302            
;;;303            *p2++ = m0 + m1;
000128  ee321a01          VADD.F32 s2,s4,s2
;;;304            *p2++ = m2 - m3;
;;;305            
;;;306            m0 = t4[2] * twI;
;;;307            m1 = t4[3] * twR;
00012c  ee232a80          VMUL.F32 s4,s7,s0
;;;308            m2 = t4[3] * twI;
;;;309            m3 = t4[2] * twR;
000130  ee210a80          VMUL.F32 s0,s3,s0
000134  ed801a02          VSTR     s2,[r0,#8]            ;303
000138  ee321ac3          VSUB.F32 s2,s5,s6              ;304
;;;310            
;;;311            *pMid2++ = m0 - m1;
;;;312            *pMid2++ = m2 + m3;
00013c  3110              ADDS     r1,r1,#0x10
00013e  3210              ADDS     r2,r2,#0x10
000140  ed801a03          VSTR     s2,[r0,#0xc]          ;304
000144  ee211aa0          VMUL.F32 s2,s3,s1              ;306
000148  ee630aa0          VMUL.F32 s1,s7,s1              ;308
00014c  3010              ADDS     r0,r0,#0x10
00014e  3610              ADDS     r6,r6,#0x10
000150  ee311a42          VSUB.F32 s2,s2,s4              ;311
000154  ee300a80          VADD.F32 s0,s1,s0
000158  ed831a02          VSTR     s2,[r3,#8]            ;311
00015c  ed830a03          VSTR     s0,[r3,#0xc]
000160  3310              ADDS     r3,r3,#0x10
000162  1e7f              SUBS     r7,r7,#1
000164  f47faf5e          BNE      |L2.36|
                  |L2.360|
;;;313        }
;;;314    
;;;315        // first col
;;;316        arm_radix8_butterfly_f32( pCol1, L, (float32_t *) S->pTwiddle, 2u);
000168  4621              MOV      r1,r4
00016a  2302              MOVS     r3,#2
00016c  4660              MOV      r0,r12
00016e  686a              LDR      r2,[r5,#4]
000170  f7fffffe          BL       arm_radix8_butterfly_f32
;;;317        // second col
;;;318        arm_radix8_butterfly_f32( pCol2, L, (float32_t *) S->pTwiddle, 2u);
000174  686a              LDR      r2,[r5,#4]
000176  4621              MOV      r1,r4
000178  ecbd8b02          VPOP     {d8}
00017c  4640              MOV      r0,r8
00017e  e8bd41f0          POP      {r4-r8,lr}
000182  2302              MOVS     r3,#2
000184  f7ffbffe          B.W      arm_radix8_butterfly_f32
;;;319    }
;;;320    
                          ENDP


                          AREA ||i.arm_cfft_radix8by4_f32||, CODE, READONLY, ALIGN=1

                  arm_cfft_radix8by4_f32 PROC
;;;320    
;;;321    void arm_cfft_radix8by4_f32( arm_cfft_instance_f32 * S, float32_t * p1) 
000000  e92d4ff3          PUSH     {r0,r1,r4-r11,lr}
;;;322    {
000004  ed2d8b04          VPUSH    {d8-d9}
000008  b087              SUB      sp,sp,#0x1c
;;;323        uint32_t    L  = S->fftLen >> 1;
00000a  980b              LDR      r0,[sp,#0x2c]
00000c  8800              LDRH     r0,[r0,#0]
00000e  ea4f0850          LSR      r8,r0,#1
;;;324        float32_t * pCol1, *pCol2, *pCol3, *pCol4, *pEnd1, *pEnd2, *pEnd3, *pEnd4;
;;;325        const float32_t *tw2, *tw3, *tw4;
;;;326        float32_t * p2 = p1 + L;
000012  eb010088          ADD      r0,r1,r8,LSL #2
;;;327        float32_t * p3 = p2 + L;
000016  eb000388          ADD      r3,r0,r8,LSL #2
;;;328        float32_t * p4 = p3 + L;
00001a  eb030488          ADD      r4,r3,r8,LSL #2
;;;329        float32_t t2[4], t3[4], t4[4], twR, twI;
;;;330        float32_t p1ap3_0, p1sp3_0, p1ap3_1, p1sp3_1;
;;;331        float32_t m0, m1, m2, m3;
;;;332        uint32_t l, twMod2, twMod3, twMod4;
;;;333    
;;;334        pCol1 = p1;         // points to real values by default
;;;335        pCol2 = p2;
;;;336        pCol3 = p3;
;;;337        pCol4 = p4;
;;;338        pEnd1 = p2 - 1;     // points to imaginary values by default
;;;339        pEnd2 = p3 - 1;
;;;340        pEnd3 = p4 - 1;
;;;341        pEnd4 = pEnd3 + L;
00001e  e9cd4300          STRD     r4,r3,[sp,#0]
;;;342    
;;;343        tw2 = tw3 = tw4 = (float32_t *) S->pTwiddle;
000022  f8ddc02c          LDR      r12,[sp,#0x2c]
000026  1f22              SUBS     r2,r4,#4              ;340
000028  eb020788          ADD      r7,r2,r8,LSL #2       ;341
00002c  e9cd0102          STRD     r0,r1,[sp,#8]         ;341
;;;344    
;;;345        L >>= 1;
000030  ea4f0858          LSR      r8,r8,#1
000034  f8dca004          LDR      r10,[r12,#4]
;;;346    
;;;347        // do four dot Fourier transform
;;;348    
;;;349        twMod2 = 2;
;;;350        twMod3 = 4;
000038  f8cd8010          STR      r8,[sp,#0x10]
00003c  f04f0804          MOV      r8,#4
;;;351        twMod4 = 6;
000040  f8cd8018          STR      r8,[sp,#0x18]
000044  f04f0806          MOV      r8,#6
;;;352    
;;;353        // TOP
;;;354        p1ap3_0 = p1[0] + p3[0];
000048  f8cd8014          STR      r8,[sp,#0x14]
00004c  edd10a00          VLDR     s1,[r1,#0]
000050  ed930a00          VLDR     s0,[r3,#0]
;;;355        p1sp3_0 = p1[0] - p3[0];
;;;356        p1ap3_1 = p1[1] + p3[1];
;;;357        p1sp3_1 = p1[1] - p3[1];
;;;358    
;;;359        // col 2
;;;360        t2[0] = p1sp3_0 + p2[1] - p4[1];
000054  edd03a01          VLDR     s7,[r0,#4]
000058  ed943a01          VLDR     s6,[r4,#4]
00005c  ee302a80          VADD.F32 s4,s1,s0              ;354
000060  ee701ac0          VSUB.F32 s3,s1,s0              ;355
000064  edd10a01          VLDR     s1,[r1,#4]            ;356
000068  ed930a01          VLDR     s0,[r3,#4]            ;356
00006c  1f06              SUBS     r6,r0,#4              ;338
00006e  1f1d              SUBS     r5,r3,#4              ;339
000070  ee702a80          VADD.F32 s5,s1,s0              ;356
000074  ee301ac0          VSUB.F32 s2,s1,s0              ;357
000078  ee330aa1          VADD.F32 s0,s7,s3
;;;361        t2[1] = p1sp3_1 - p2[0] + p4[0];
00007c  edd40a00          VLDR     s1,[r4,#0]
;;;362        // col 3
;;;363        t3[0] = p1ap3_0 - p2[0] - p4[0];
;;;364        t3[1] = p1ap3_1 - p2[1] - p4[1];
;;;365        // col 4
;;;366        t4[0] = p1sp3_0 - p2[1] + p4[1];
000080  ee711ae3          VSUB.F32 s3,s3,s7
;;;367        t4[1] = p1sp3_1 + p2[0] - p4[0];
;;;368        // col 1
;;;369        *p1++ = p1ap3_0 + p2[0] + p4[0];
;;;370        *p1++ = p1ap3_1 + p2[1] + p4[1];
;;;371    
;;;372        // Twiddle factors are ones
;;;373        *p2++ = t2[0];
;;;374        *p2++ = t2[1];
;;;375        *p3++ = t3[0];
;;;376        *p3++ = t3[1];
;;;377        *p4++ = t4[0];
;;;378        *p4++ = t4[1];
;;;379    
;;;380        tw2 += twMod2;
000084  f10a0b08          ADD      r11,r10,#8
000088  ee705a43          VSUB.F32 s11,s0,s6             ;360
00008c  ed900a00          VLDR     s0,[r0,#0]            ;361
000090  ee711a83          VADD.F32 s3,s3,s6              ;366
;;;381        tw3 += twMod3;
000094  f10a0910          ADD      r9,r10,#0x10
000098  ee314a40          VSUB.F32 s8,s2,s0              ;361
00009c  ee724a40          VSUB.F32 s9,s4,s0              ;363
0000a0  ee301a01          VADD.F32 s2,s0,s2              ;367
0000a4  ee300a02          VADD.F32 s0,s0,s4              ;369
0000a8  ee344a20          VADD.F32 s8,s8,s1              ;361
0000ac  ee345ae0          VSUB.F32 s10,s9,s1             ;363
0000b0  ee311a60          VSUB.F32 s2,s2,s1              ;367
0000b4  ee300a20          VADD.F32 s0,s0,s1              ;369
0000b8  ee724ae3          VSUB.F32 s9,s5,s7              ;364
;;;382        tw4 += twMod4;
0000bc  f10a0a18          ADD      r10,r10,#0x18
0000c0  ed810a00          VSTR     s0,[r1,#0]            ;369
0000c4  ed900a01          VLDR     s0,[r0,#4]            ;370
0000c8  edd40a01          VLDR     s1,[r4,#4]            ;370
0000cc  ee744ac3          VSUB.F32 s9,s9,s6              ;364
0000d0  ee300a22          VADD.F32 s0,s0,s5              ;370
0000d4  ee300a20          VADD.F32 s0,s0,s1              ;370
0000d8  ed810a01          VSTR     s0,[r1,#4]            ;370
0000dc  edc05a00          VSTR     s11,[r0,#0]           ;373
0000e0  ed804a01          VSTR     s8,[r0,#4]            ;374
0000e4  ed835a00          VSTR     s10,[r3,#0]           ;375
0000e8  edc34a01          VSTR     s9,[r3,#4]            ;376
0000ec  edc41a00          VSTR     s3,[r4,#0]            ;377
0000f0  ed841a01          VSTR     s2,[r4,#4]            ;378
;;;383    
;;;384        for (l = (L - 2) >> 1; l > 0; l-- ) 
0000f4  f8dd8010          LDR      r8,[sp,#0x10]
0000f8  3008              ADDS     r0,r0,#8
0000fa  f1a80802          SUB      r8,r8,#2
0000fe  3108              ADDS     r1,r1,#8
000100  3308              ADDS     r3,r3,#8
000102  ea4f0c58          LSR      r12,r8,#1
000106  3408              ADDS     r4,r4,#8
                  |L3.264|
;;;385        {
;;;386            // TOP
;;;387            p1ap3_0 = p1[0] + p3[0];
;;;388            p1sp3_0 = p1[0] - p3[0];
;;;389            p1ap3_1 = p1[1] + p3[1];
;;;390            p1sp3_1 = p1[1] - p3[1];
;;;391            // col 2
;;;392            t2[0] = p1sp3_0 + p2[1] - p4[1];
;;;393            t2[1] = p1sp3_1 - p2[0] + p4[0];
;;;394            // col 3
;;;395            t3[0] = p1ap3_0 - p2[0] - p4[0];
;;;396            t3[1] = p1ap3_1 - p2[1] - p4[1];
;;;397            // col 4
;;;398            t4[0] = p1sp3_0 - p2[1] + p4[1];
;;;399            t4[1] = p1sp3_1 + p2[0] - p4[0];
;;;400            // col 1 - top
;;;401            *p1++ = p1ap3_0 + p2[0] + p4[0];
;;;402            *p1++ = p1ap3_1 + p2[1] + p4[1];
;;;403    
;;;404            // BOTTOM
;;;405            p1ap3_1 = pEnd1[-1] + pEnd3[-1];
;;;406            p1sp3_1 = pEnd1[-1] - pEnd3[-1];
;;;407            p1ap3_0 = pEnd1[0] + pEnd3[0];
;;;408            p1sp3_0 = pEnd1[0] - pEnd3[0];
;;;409            // col 2
;;;410            t2[2] = pEnd2[0]  - pEnd4[0] + p1sp3_1;
;;;411            t2[3] = pEnd1[0] - pEnd3[0] - pEnd2[-1] + pEnd4[-1];
;;;412            // col 3
;;;413            t3[2] = p1ap3_1 - pEnd2[-1] - pEnd4[-1];
;;;414            t3[3] = p1ap3_0 - pEnd2[0]  - pEnd4[0];
;;;415            // col 4
;;;416            t4[2] = pEnd2[0]  - pEnd4[0]  - p1sp3_1;
;;;417            t4[3] = pEnd4[-1] - pEnd2[-1] - p1sp3_0;
;;;418            // col 1 - Bottom
;;;419            *pEnd1-- = p1ap3_0 + pEnd2[0] + pEnd4[0];
;;;420            *pEnd1-- = p1ap3_1 + pEnd2[-1] + pEnd4[-1];
;;;421    
;;;422            // COL 2
;;;423            // read twiddle factors
;;;424            twR = *tw2++;
;;;425            twI = *tw2++;
;;;426            // multiply by twiddle factors
;;;427            //  let    Z1 = a + i(b),   Z2 = c + i(d)
;;;428            //   =>  Z1 * Z2  =  (a*c - b*d) + i(b*c + a*d)
;;;429            
;;;430            // Top
;;;431            m0 = t2[0] * twR;
;;;432            m1 = t2[1] * twI;
;;;433            m2 = t2[1] * twR;
;;;434            m3 = t2[0] * twI;
;;;435            
;;;436            *p2++ = m0 + m1;
;;;437            *p2++ = m2 - m3;
;;;438            // use vertical symmetry col 2
;;;439            // 0.9997 - 0.0245i  <==>  0.0245 - 0.9997i
;;;440            // Bottom
;;;441            m0 = t2[3] * twI;
;;;442            m1 = t2[2] * twR;
;;;443            m2 = t2[2] * twI;
;;;444            m3 = t2[3] * twR;
;;;445            
;;;446            *pEnd2-- = m0 - m1;
;;;447            *pEnd2-- = m2 + m3;
;;;448    
;;;449            // COL 3
;;;450            twR = tw3[0];
;;;451            twI = tw3[1];
;;;452            tw3 += twMod3;
;;;453            // Top
;;;454            m0 = t3[0] * twR;
;;;455            m1 = t3[1] * twI;
;;;456            m2 = t3[1] * twR;
;;;457            m3 = t3[0] * twI;
;;;458            
;;;459            *p3++ = m0 + m1;
;;;460            *p3++ = m2 - m3;
;;;461            // use vertical symmetry col 3
;;;462            // 0.9988 - 0.0491i  <==>  -0.9988 - 0.0491i
;;;463            // Bottom
;;;464            m0 = -t3[3] * twR;
;;;465            m1 = t3[2] * twI;
;;;466            m2 = t3[2] * twR;
;;;467            m3 = t3[3] * twI;
;;;468            
;;;469            *pEnd3-- = m0 - m1;
;;;470            *pEnd3-- = m3 - m2;
;;;471            
;;;472            // COL 4
;;;473            twR = tw4[0];
;;;474            twI = tw4[1];
;;;475            tw4 += twMod4;
;;;476            // Top
;;;477            m0 = t4[0] * twR;
;;;478            m1 = t4[1] * twI;
;;;479            m2 = t4[1] * twR;
;;;480            m3 = t4[0] * twI;
;;;481            
;;;482            *p4++ = m0 + m1;
;;;483            *p4++ = m2 - m3;
;;;484            // use vertical symmetry col 4
;;;485            // 0.9973 - 0.0736i  <==>  -0.0736 + 0.9973i
;;;486            // Bottom
;;;487            m0 = t4[3] * twI;
;;;488            m1 = t4[2] * twR;
;;;489            m2 = t4[2] * twI;
;;;490            m3 = t4[3] * twR;
;;;491            
;;;492            *pEnd4-- = m0 - m1;
;;;493            *pEnd4-- = m2 + m3;
;;;494        }
;;;495    
;;;496        //MIDDLE
;;;497        // Twiddle factors are 
;;;498        //  1.0000  0.7071-0.7071i  -1.0000i  -0.7071-0.7071i
;;;499        p1ap3_0 = p1[0] + p3[0];
000108  edd10a00          VLDR     s1,[r1,#0]
00010c  ed930a00          VLDR     s0,[r3,#0]
000110  ea5f0e0c          MOVS     lr,r12                ;384
000114  ee704a80          VADD.F32 s9,s1,s0
;;;500        p1sp3_0 = p1[0] - p3[0];
000118  ee701ac0          VSUB.F32 s3,s1,s0
;;;501        p1ap3_1 = p1[1] + p3[1];
00011c  edd10a01          VLDR     s1,[r1,#4]
000120  ed930a01          VLDR     s0,[r3,#4]
000124  d07e              BEQ      |L3.548|
000126  ed905a01          VLDR     s10,[r0,#4]           ;392
00012a  ee302a80          VADD.F32 s4,s1,s0              ;389
00012e  ee301ac0          VSUB.F32 s2,s1,s0              ;390
000132  ee350a21          VADD.F32 s0,s10,s3             ;392
000136  edd43a01          VLDR     s7,[r4,#4]            ;392
00013a  edd00a00          VLDR     s1,[r0,#0]            ;393
00013e  ee711ac5          VSUB.F32 s3,s3,s10             ;398
000142  ee705a63          VSUB.F32 s11,s0,s7             ;392
000146  ee712a60          VSUB.F32 s5,s2,s1              ;393
00014a  ed940a00          VLDR     s0,[r4,#0]            ;393
00014e  ee301a81          VADD.F32 s2,s1,s2              ;399
000152  ee711aa3          VADD.F32 s3,s3,s7              ;398
000156  ee324a80          VADD.F32 s8,s5,s0              ;393
00015a  ee742ae0          VSUB.F32 s5,s9,s1              ;395
00015e  ee700aa4          VADD.F32 s1,s1,s9              ;401
000162  ee311a40          VSUB.F32 s2,s2,s0              ;399
000166  ee323ac0          VSUB.F32 s6,s5,s0              ;395
00016a  ee300a80          VADD.F32 s0,s1,s0              ;401
00016e  ee722a45          VSUB.F32 s5,s4,s10             ;396
000172  ed810a00          VSTR     s0,[r1,#0]            ;401
000176  ed900a01          VLDR     s0,[r0,#4]            ;402
00017a  edd40a01          VLDR     s1,[r4,#4]            ;402
00017e  ee722ae3          VSUB.F32 s5,s5,s7              ;396
000182  ee300a02          VADD.F32 s0,s0,s4              ;402
000186  ee300a20          VADD.F32 s0,s0,s1              ;402
00018a  ed810a01          VSTR     s0,[r1,#4]            ;402
00018e  ed560a01          VLDR     s1,[r6,#-4]           ;405
000192  ed120a01          VLDR     s0,[r2,#-4]           ;405
000196  ed155a01          VLDR     s10,[r5,#-4]          ;411
00019a  ed577a01          VLDR     s15,[r7,#-4]          ;411
00019e  ee708a80          VADD.F32 s17,s1,s0             ;405
0001a2  ee302ac0          VSUB.F32 s4,s1,s0              ;406
0001a6  edd60a00          VLDR     s1,[r6,#0]            ;407
0001aa  ed920a00          VLDR     s0,[r2,#0]            ;407
0001ae  ed957a00          VLDR     s14,[r5,#0]           ;410
0001b2  ee308a80          VADD.F32 s16,s1,s0             ;407
0001b6  ee300ac0          VSUB.F32 s0,s1,s0              ;408
0001ba  edd70a00          VLDR     s1,[r7,#0]            ;410
0001be  ee703a45          VSUB.F32 s7,s0,s10             ;411
0001c2  ee379a60          VSUB.F32 s18,s14,s1            ;410
0001c6  ee736aa7          VADD.F32 s13,s7,s15            ;411
0001ca  ee783ac5          VSUB.F32 s7,s17,s10            ;413
0001ce  ee375ac5          VSUB.F32 s10,s15,s10           ;417
0001d2  ee396a02          VADD.F32 s12,s18,s4            ;410
0001d6  ee392a42          VSUB.F32 s4,s18,s4             ;416
0001da  ee734ae7          VSUB.F32 s9,s7,s15             ;413
0001de  ee355a40          VSUB.F32 s10,s10,s0            ;417
0001e2  ee370a08          VADD.F32 s0,s14,s16            ;419
0001e6  ee783a47          VSUB.F32 s7,s16,s14            ;414
0001ea  ee300a20          VADD.F32 s0,s0,s1              ;419
0001ee  ee733ae0          VSUB.F32 s7,s7,s1              ;414
0001f2  ed860a00          VSTR     s0,[r6,#0]            ;419
0001f6  ed150a01          VLDR     s0,[r5,#-4]           ;420
0001fa  ed570a01          VLDR     s1,[r7,#-4]           ;420
0001fe  ee300a28          VADD.F32 s0,s0,s17             ;420
000202  ee300a20          VADD.F32 s0,s0,s1              ;420
000206  ed060a01          VSTR     s0,[r6,#-4]           ;420
00020a  ed9b0a00          VLDR     s0,[r11,#0]           ;424
00020e  eddb0a01          VLDR     s1,[r11,#4]           ;425
000212  f10b0b08          ADD      r11,r11,#8            ;425
000216  ee257a80          VMUL.F32 s14,s11,s0            ;431
00021a  ee647a20          VMUL.F32 s15,s8,s1             ;432
00021e  ee248a00          VMUL.F32 s16,s8,s0             ;433
000222  e000              B        |L3.550|
                  |L3.548|
000224  e074              B        |L3.784|
                  |L3.550|
000226  ee254aa0          VMUL.F32 s8,s11,s1             ;434
00022a  ee775a27          VADD.F32 s11,s14,s15           ;436
00022e  3108              ADDS     r1,r1,#8              ;384
000230  3e08              SUBS     r6,r6,#8              ;384
000232  edc05a00          VSTR     s11,[r0,#0]           ;436
000236  ee384a44          VSUB.F32 s8,s16,s8             ;437
00023a  ee665aa0          VMUL.F32 s11,s13,s1            ;441
00023e  ee660a20          VMUL.F32 s1,s12,s1             ;443
000242  ed804a01          VSTR     s8,[r0,#4]            ;437
000246  ee264a00          VMUL.F32 s8,s12,s0             ;442
00024a  ee260a80          VMUL.F32 s0,s13,s0             ;444
00024e  3008              ADDS     r0,r0,#8              ;384
000250  f1ae0c01          SUB      r12,lr,#1             ;384
000254  ee354ac4          VSUB.F32 s8,s11,s8             ;446
000258  ee300a80          VADD.F32 s0,s1,s0              ;447
00025c  ed854a00          VSTR     s8,[r5,#0]            ;446
000260  ed050a01          VSTR     s0,[r5,#-4]           ;447
000264  edd90a00          VLDR     s1,[r9,#0]            ;450
000268  ed990a01          VLDR     s0,[r9,#4]            ;451
00026c  f8dd8018          LDR      r8,[sp,#0x18]         ;452
000270  ee234a20          VMUL.F32 s8,s6,s1              ;454
000274  ee625a80          VMUL.F32 s11,s5,s0             ;455
000278  ee622aa0          VMUL.F32 s5,s5,s1              ;456
00027c  ee233a00          VMUL.F32 s6,s6,s0              ;457
000280  eb090988          ADD      r9,r9,r8,LSL #2       ;452
000284  ee344a25          VADD.F32 s8,s8,s11             ;459
000288  3d08              SUBS     r5,r5,#8              ;384
00028a  ee722ac3          VSUB.F32 s5,s5,s6              ;460
00028e  ed834a00          VSTR     s8,[r3,#0]            ;459
000292  ee233ae0          VNMUL.F32 s6,s7,s1              ;464
000296  edc32a01          VSTR     s5,[r3,#4]            ;460
00029a  ee642a80          VMUL.F32 s5,s9,s0              ;465
00029e  ee640aa0          VMUL.F32 s1,s9,s1              ;466
0002a2  ee230a80          VMUL.F32 s0,s7,s0              ;467
0002a6  3308              ADDS     r3,r3,#8              ;384
0002a8  ee732a62          VSUB.F32 s5,s6,s5              ;469
0002ac  ee300a60          VSUB.F32 s0,s0,s1              ;470
0002b0  edc22a00          VSTR     s5,[r2,#0]            ;469
0002b4  ed020a01          VSTR     s0,[r2,#-4]           ;470
0002b8  edda0a00          VLDR     s1,[r10,#0]           ;473
0002bc  ed9a0a01          VLDR     s0,[r10,#4]           ;474
0002c0  f8dd8014          LDR      r8,[sp,#0x14]         ;475
0002c4  ee213aa0          VMUL.F32 s6,s3,s1              ;477
0002c8  ee612a00          VMUL.F32 s5,s2,s0              ;478
0002cc  ee613a20          VMUL.F32 s7,s2,s1              ;479
0002d0  ee211a80          VMUL.F32 s2,s3,s0              ;480
0002d4  3a08              SUBS     r2,r2,#8              ;384
0002d6  ee731a22          VADD.F32 s3,s6,s5              ;482
0002da  eb0a0a88          ADD      r10,r10,r8,LSL #2     ;475
0002de  ee331ac1          VSUB.F32 s2,s7,s2              ;483
0002e2  edc41a00          VSTR     s3,[r4,#0]            ;482
0002e6  ee621a20          VMUL.F32 s3,s4,s1              ;488
0002ea  ed841a01          VSTR     s2,[r4,#4]            ;483
0002ee  ee251a00          VMUL.F32 s2,s10,s0             ;487
0002f2  ee220a00          VMUL.F32 s0,s4,s0              ;489
0002f6  ee650a20          VMUL.F32 s1,s10,s1             ;490
0002fa  3408              ADDS     r4,r4,#8              ;384
0002fc  ee311a61          VSUB.F32 s2,s2,s3              ;492
000300  ee300a20          VADD.F32 s0,s0,s1              ;493
000304  ed871a00          VSTR     s2,[r7,#0]            ;492
000308  ed070a01          VSTR     s0,[r7,#-4]           ;493
00030c  3f08              SUBS     r7,r7,#8              ;384
00030e  e6fb              B        |L3.264|
                  |L3.784|
000310  ee305a80          VADD.F32 s10,s1,s0
;;;502        p1sp3_1 = p1[1] - p3[1];
;;;503    
;;;504        // col 2
;;;505        t2[0] = p1sp3_0 + p2[1] - p4[1];
000314  ed901a01          VLDR     s2,[r0,#4]
000318  ee705ac0          VSUB.F32 s11,s1,s0             ;502
00031c  ed942a01          VLDR     s4,[r4,#4]
000320  ee310a21          VADD.F32 s0,s2,s3
;;;506        t2[1] = p1sp3_1 - p2[0] + p4[0];
000324  edd00a00          VLDR     s1,[r0,#0]
;;;507        // col 3
;;;508        t3[0] = p1ap3_0 - p2[0] - p4[0];
;;;509        t3[1] = p1ap3_1 - p2[1] - p4[1];
000328  ee354a41          VSUB.F32 s8,s10,s2
;;;510        // col 4
;;;511        t4[0] = p1sp3_0 - p2[1] + p4[1];
00032c  ee311ac1          VSUB.F32 s2,s3,s2
000330  ee702a42          VSUB.F32 s5,s0,s4              ;505
000334  ed940a00          VLDR     s0,[r4,#0]            ;506
000338  ee353ae0          VSUB.F32 s6,s11,s1             ;506
00033c  ee743ae0          VSUB.F32 s7,s9,s1              ;508
;;;512        t4[1] = p1sp3_1 + p2[0] - p4[0];
000340  ee701aa5          VADD.F32 s3,s1,s11
;;;513        // col 1 - Top
;;;514        *p1++ = p1ap3_0 + p2[0] + p4[0];
000344  ee700aa4          VADD.F32 s1,s1,s9
000348  ee333a00          VADD.F32 s6,s6,s0              ;506
00034c  ee733ac0          VSUB.F32 s7,s7,s0              ;508
000350  ee711ac0          VSUB.F32 s3,s3,s0              ;512
000354  ee300a80          VADD.F32 s0,s1,s0
000358  ee344a42          VSUB.F32 s8,s8,s4              ;509
00035c  ee311a02          VADD.F32 s2,s2,s4              ;511
000360  ed810a00          VSTR     s0,[r1,#0]
;;;515        *p1++ = p1ap3_1 + p2[1] + p4[1];
000364  ed900a01          VLDR     s0,[r0,#4]
000368  edd40a01          VLDR     s1,[r4,#4]
00036c  ee300a05          VADD.F32 s0,s0,s10
000370  ee300a20          VADD.F32 s0,s0,s1
000374  ed810a01          VSTR     s0,[r1,#4]
000378  ec9b0a02          VLDM     r11,{s0-s1}
;;;516    
;;;517        // COL 2
;;;518        twR = tw2[0];
;;;519        twI = tw2[1];
;;;520    
;;;521        m0 = t2[0] * twR;
00037c  ee624a80          VMUL.F32 s9,s5,s0
;;;522        m1 = t2[1] * twI;
000380  ee232a20          VMUL.F32 s4,s6,s1
;;;523        m2 = t2[1] * twR;
000384  ee230a00          VMUL.F32 s0,s6,s0
;;;524        m3 = t2[0] * twI;
000388  ee620aa0          VMUL.F32 s1,s5,s1
;;;525    
;;;526        *p2++ = m0 + m1;
00038c  ee342a82          VADD.F32 s4,s9,s4
;;;527        *p2++ = m2 - m3;
000390  ee300a60          VSUB.F32 s0,s0,s1
000394  ed802a00          VSTR     s4,[r0,#0]            ;526
000398  ed800a01          VSTR     s0,[r0,#4]
;;;528        // COL 3
;;;529        twR = tw3[0];
00039c  edd90a00          VLDR     s1,[r9,#0]
;;;530        twI = tw3[1];
0003a0  ed990a01          VLDR     s0,[r9,#4]
;;;531    
;;;532        m0 = t3[0] * twR;
0003a4  ee232aa0          VMUL.F32 s4,s7,s1
;;;533        m1 = t3[1] * twI;
0003a8  ee642a00          VMUL.F32 s5,s8,s0
;;;534        m2 = t3[1] * twR;
0003ac  ee640a20          VMUL.F32 s1,s8,s1
;;;535        m3 = t3[0] * twI;
0003b0  ee230a80          VMUL.F32 s0,s7,s0
;;;536    
;;;537        *p3++ = m0 + m1;
0003b4  ee322a22          VADD.F32 s4,s4,s5
;;;538        *p3++ = m2 - m3;
0003b8  ee300ac0          VSUB.F32 s0,s1,s0
0003bc  ed832a00          VSTR     s4,[r3,#0]            ;537
0003c0  ed830a01          VSTR     s0,[r3,#4]
;;;539        // COL 4
;;;540        twR = tw4[0];
;;;541        twI = tw4[1];
;;;542    
;;;543        m0 = t4[0] * twR;
;;;544        m1 = t4[1] * twI;
;;;545        m2 = t4[1] * twR;
;;;546        m3 = t4[0] * twI;
;;;547    
;;;548        *p4++ = m0 + m1;
;;;549        *p4++ = m2 - m3;
;;;550    
;;;551        // first col
;;;552        arm_radix8_butterfly_f32( pCol1, L, (float32_t *) S->pTwiddle, 4u);
0003c4  2304              MOVS     r3,#4
0003c6  ec9a0a02          VLDM     r10,{s0-s1}           ;538
0003ca  ee212a00          VMUL.F32 s4,s2,s0              ;543
0003ce  ee612aa0          VMUL.F32 s5,s3,s1              ;544
0003d2  ee210a80          VMUL.F32 s0,s3,s0              ;545
0003d6  ee610a20          VMUL.F32 s1,s2,s1              ;546
0003da  ee321a22          VADD.F32 s2,s4,s5              ;548
0003de  ee300a60          VSUB.F32 s0,s0,s1              ;549
0003e2  ed841a00          VSTR     s2,[r4,#0]            ;548
0003e6  ed840a01          VSTR     s0,[r4,#4]            ;549
0003ea  980b              LDR      r0,[sp,#0x2c]
0003ec  6842              LDR      r2,[r0,#4]
0003ee  e9dd0103          LDRD     r0,r1,[sp,#0xc]
0003f2  f7fffffe          BL       arm_radix8_butterfly_f32
;;;553        // second col
;;;554        arm_radix8_butterfly_f32( pCol2, L, (float32_t *) S->pTwiddle, 4u);
0003f6  980b              LDR      r0,[sp,#0x2c]
0003f8  2304              MOVS     r3,#4
0003fa  9904              LDR      r1,[sp,#0x10]
0003fc  6842              LDR      r2,[r0,#4]
0003fe  9802              LDR      r0,[sp,#8]
000400  f7fffffe          BL       arm_radix8_butterfly_f32
;;;555        // third col
;;;556        arm_radix8_butterfly_f32( pCol3, L, (float32_t *) S->pTwiddle, 4u);
000404  980b              LDR      r0,[sp,#0x2c]
000406  2304              MOVS     r3,#4
000408  9904              LDR      r1,[sp,#0x10]
00040a  6842              LDR      r2,[r0,#4]
00040c  9801              LDR      r0,[sp,#4]
00040e  f7fffffe          BL       arm_radix8_butterfly_f32
;;;557        // fourth col
;;;558        arm_radix8_butterfly_f32( pCol4, L, (float32_t *) S->pTwiddle, 4u);
000412  980b              LDR      r0,[sp,#0x2c]
000414  9904              LDR      r1,[sp,#0x10]
000416  2304              MOVS     r3,#4
000418  6842              LDR      r2,[r0,#4]
00041a  9800              LDR      r0,[sp,#0]
00041c  b007              ADD      sp,sp,#0x1c
00041e  ecbd8b04          VPOP     {d8-d9}
000422  b002              ADD      sp,sp,#8
000424  e8bd4ff0          POP      {r4-r11,lr}
000428  f7ffbffe          B.W      arm_radix8_butterfly_f32
;;;559    }
;;;560    
                          ENDP


;*** Start embedded assembler ***

#line 1 "src\\TransformFunctions\\arm_cfft_f32.c"
	AREA ||.rev16_text||, CODE
	THUMB
	EXPORT |__asm___14_arm_cfft_f32_c_4a34056b____REV16|
#line 129 ".\\inc\\core_cmInstr.h"
|__asm___14_arm_cfft_f32_c_4a34056b____REV16| PROC
#line 130

 rev16 r0, r0
 bx lr
	ENDP
	AREA ||.revsh_text||, CODE
	THUMB
	EXPORT |__asm___14_arm_cfft_f32_c_4a34056b____REVSH|
#line 144
|__asm___14_arm_cfft_f32_c_4a34056b____REVSH| PROC
#line 145

 revsh r0, r0
 bx lr
	ENDP
	AREA ||.rrx_text||, CODE
	THUMB
	EXPORT |__asm___14_arm_cfft_f32_c_4a34056b____RRX|
#line 300
|__asm___14_arm_cfft_f32_c_4a34056b____RRX| PROC
#line 301

 rrx r0, r0
 bx lr
	ENDP

;*** End   embedded assembler ***
